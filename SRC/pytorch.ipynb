{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7eYtiq9oeO-"
      },
      "source": [
        "<!-- Autogenerated by `scripts/make_examples.py` -->\n",
        "<table align=\"left\">\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb\">\n",
        "            <img src=\"https://user-images.githubusercontent.com/25985824/104791629-6e618700-5769-11eb-857f-d176b37d2496.png\" height=\"32\" width=\"32\">\n",
        "            Try in Google Colab\n",
        "        </a>\n",
        "    </td>\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb\">\n",
        "            <img src=\"https://user-images.githubusercontent.com/25985824/104791634-6efa1d80-5769-11eb-8a4c-71d6cb53ccf0.png\" height=\"32\" width=\"32\">\n",
        "            Share via nbviewer\n",
        "        </a>\n",
        "    </td>\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://github.com/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb\">\n",
        "            <img src=\"https://user-images.githubusercontent.com/25985824/104791633-6efa1d80-5769-11eb-8ee3-4b2123fe4b66.png\" height=\"32\" width=\"32\">\n",
        "            View on GitHub\n",
        "        </a>\n",
        "    </td>\n",
        "    <td>\n",
        "        <a href=\"https://github.com/voxel51/fiftyone-examples/raw/master/examples/pytorch_detection_training.ipynb\" download>\n",
        "            <img src=\"https://user-images.githubusercontent.com/25985824/104792428-60f9cc00-576c-11eb-95a4-5709d803023a.png\" height=\"32\" width=\"32\">\n",
        "            Download notebook\n",
        "        </a>\n",
        "    </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtvfsp3wWRH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa48c2f-752c-4e71-9886-e6bd226c2d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fiftyone in /usr/local/lib/python3.10/dist-packages (0.22.2)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.2.1)\n",
            "Requirement already satisfied: argcomplete in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.11.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.28.68)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.3.1)\n",
            "Requirement already satisfied: dacite<1.8.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.7.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.14)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.1.1)\n",
            "Requirement already satisfied: hypercorn>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.14.4)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.2)\n",
            "Requirement already satisfied: kaleido!=0.2.1.post1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Requirement already satisfied: mongoengine==0.24.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.24.2)\n",
            "Requirement already satisfied: motor>=2.5 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (23.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.3)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (9.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.15.0)\n",
            "Requirement already satisfied: pprintpp in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Requirement already satisfied: pymongo>=3.12 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.5.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.3.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2023.6.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.3.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.19.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (67.7.2)\n",
            "Requirement already satisfied: sseclient-py<2,>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.8.0)\n",
            "Requirement already satisfied: sse-starlette<1,>=0.10.3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.10.3)\n",
            "Requirement already satisfied: starlette>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.31.1)\n",
            "Requirement already satisfied: strawberry-graphql==0.138.1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.138.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.9.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.13.0)\n",
            "Requirement already satisfied: universal-analytics-python3<2,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.1.1)\n",
            "Requirement already satisfied: fiftyone-brain~=0.13.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.13.2)\n",
            "Requirement already satisfied: fiftyone-db~=0.4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.4.0)\n",
            "Requirement already satisfied: voxel51-eta~=0.12 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.12.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.8.1.78)\n",
            "Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fiftyone-brain~=0.13.2->fiftyone) (1.11.3)\n",
            "Requirement already satisfied: h11 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\n",
            "Requirement already satisfied: h2>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
            "Requirement already satisfied: priority in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\n",
            "Requirement already satisfied: wsproto>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (8.2.3)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo>=3.12->fiftyone) (2.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n",
            "Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.25.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.3.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.18.3)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.7)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (4.0.0)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (0.20.6)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (5.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta~=0.12->fiftyone) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fiftyone) (2.5)\n",
            "Requirement already satisfied: botocore<1.32.0,>=1.31.68 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (1.31.68)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3->fiftyone) (0.7.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.15.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.1.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.31.5)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.1.3)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (0.18.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta~=0.12->fiftyone) (23.1.0)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.7.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (3.19.0)\n",
            "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.15.9)\n",
            "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.0)\n",
            "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.1)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.2.3)\n",
            "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.1.0)\n",
            "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta~=0.12->fiftyone) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fiftyone-db-ubuntu2204"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvn_vivioUeW",
        "outputId": "9f35c86c-0987-4f09-b087-2219ca7f85be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fiftyone-db-ubuntu2204 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hilygMeboePG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d6cf60-e629-4260-a957-b56250d13db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBgcEJdsoePG"
      },
      "source": [
        "We'll also need pytorch, and torchvision, as well as clone the torchvision GitHub repository to use the training and evaluation utilities provided for the [Torchvision Object Deteciton Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-the-dataset) that we are using to train a basic object detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dVQMF0SWg5I",
        "outputId": "dacf68c8-b05d-432b-ae0f-4118cfbc7fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "HEAD is now at be376084d8 version check against PyTorch's CUDA version\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZDc2VuwNDm0"
      },
      "source": [
        "## Loading your data\n",
        "\n",
        "Getting your data into FiftyOne is oftentimes actually easier than getting it into a PyTorch dataset. Additionally, once the data is in FiftyOne it is much more flexible allowing you to easily find and access even the most specific subsets of data that you can then use to train or evaluate your model.\n",
        "\n",
        "If you have data that follows a certain format on disk ([for example a directory tree for classification, the COCO detection format, or many more](https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/index.html#id1)), then you can load it into FiftyOne in [one line of code](https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/index.html).\n",
        "\n",
        "In this notebook, I am going to work with [COCO-2017](https://cocodataset.org/#home) and load it from the [FiftyOne Dataset Zoo](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ2cvwpPWXBt",
        "outputId": "38515070-25f8-4f85-dbab-f00dd6e307cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cd1e4111c50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDvJBjDk2wpy"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5crNDNsRWdPT",
        "outputId": "3d609bbf-9040-4942-9a0d-00d218ac87ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 5000/5000 [35.5s elapsed, 0s remaining, 135.0 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 5000/5000 [35.5s elapsed, 0s remaining, 135.0 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'coco-2017-validation' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-validation' created\n"
          ]
        }
      ],
      "source": [
        "fo_dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tafdataset = foz.load_zoo_dataset(\"kitti\", split=\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES3jyUJzqgiR",
        "outputId": "456dfb81-43d1-49f4-a67b-420d3542ef14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'test' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Split 'test' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'kitti' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'kitti' split 'test'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 7518/7518 [5.5s elapsed, 0s remaining, 1.7K samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 7518/7518 [5.5s elapsed, 0s remaining, 1.7K samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'kitti-test' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'kitti-test' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tafdataset2 = foz.load_zoo_dataset(\"kitti\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjluttsYxQCT",
        "outputId": "c101fd90-9462-4433-8d3c-8f60fe725557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 'train' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Split 'train' already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'kitti' split 'train'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'kitti' split 'train'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 7481/7481 [1.1m elapsed, 0s remaining, 138.1 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 7481/7481 [1.1m elapsed, 0s remaining, 138.1 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'kitti-train' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'kitti-train' created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc43DB9jNqGD"
      },
      "source": [
        "We will be needing the height and width of images later in this notebook so we need to compute metadata on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOdY2KQeGz1r"
      },
      "outputs": [],
      "source": [
        "fo_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tafdataset2.compute_metadata()"
      ],
      "metadata": {
        "id": "iUrxMqFqyVMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99mcj42PN4OE"
      },
      "source": [
        "We can create a session and visualize this dataset in the [FiftyOne App](https://voxel51.com/docs/fiftyone/user_guide/app.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "m63jPSOmCtK1",
        "outputId": "5feaae55-a081-4b33-e26c-cf007889fcb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "body, html {\n",
              "  margin: 0;\n",
              "  padding: 0;\n",
              "  width: 100%;\n",
              "}\n",
              "\n",
              "#focontainer-39a27909-4c13-492a-98b2-e1c7073a4a72 {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-39a27909-4c13-492a-98b2-e1c7073a4a72 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-39a27909-4c13-492a-98b2-e1c7073a4a72:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-39a27909-4c13-492a-98b2-e1c7073a4a72 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-39a27909-4c13-492a-98b2-e1c7073a4a72\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-39a27909-4c13-492a-98b2-e1c7073a4a72\">\n",
              "      <button id=\"foactivate-39a27909-4c13-492a-98b2-e1c7073a4a72\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "session = fo.launch_app(fo_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(tafdataset2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "dfCCbrlbyhRK",
        "outputId": "cccc1352-6406-47e2-8103-91eb54f90a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "body, html {\n",
              "  margin: 0;\n",
              "  padding: 0;\n",
              "  width: 100%;\n",
              "}\n",
              "\n",
              "#focontainer-0dd2d7f6-569b-48df-96db-34ef67bcd2b9 {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-0dd2d7f6-569b-48df-96db-34ef67bcd2b9 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-0dd2d7f6-569b-48df-96db-34ef67bcd2b9:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-0dd2d7f6-569b-48df-96db-34ef67bcd2b9 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-0dd2d7f6-569b-48df-96db-34ef67bcd2b9\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-0dd2d7f6-569b-48df-96db-34ef67bcd2b9\">\n",
              "      <button id=\"foactivate-0dd2d7f6-569b-48df-96db-34ef67bcd2b9\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8uR5eEUOCUQ"
      },
      "source": [
        "## PyTorch dataset and training setup\n",
        "\n",
        "A [PyTorch dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) is a class that defines how to load a static dataset and its labels from disk via a simple iterator interface. They differ from FiftyOne datasets which are flexible representations of your data geared towards visualization, querying, and understanding.\n",
        "\n",
        "Every PyTorch model expects data and labels to pass into it in a certain format. Before being able to write up a PyTorch dataset class, you first need to understand the format that the model requires. Namely, we need to know exactly what format the data loader is expected to output when iterating through the dataset so that we can properly define the `__getitem__` method in the PyTorch dataset.\n",
        "In this example, I am following the [Torchvision object detection tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-the-dataset) and construct a PyTorch dataset to work with their RCNN-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRR2fVMlWeKI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import fiftyone.utils.coco as fouc\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
        "\n",
        "    Args:\n",
        "        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n",
        "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
        "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the\n",
        "            desired labels to load\n",
        "        classes (None): a list of class strings that are used to define the mapping between\n",
        "            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fiftyone_dataset,\n",
        "        transforms=None,\n",
        "        gt_field=\"ground_truth\",\n",
        "        classes=None,\n",
        "    ):\n",
        "        self.samples = fiftyone_dataset\n",
        "        self.transforms = transforms\n",
        "        self.gt_field = gt_field\n",
        "\n",
        "        self.img_paths = self.samples.values(\"filepath\")\n",
        "\n",
        "        self.classes = classes\n",
        "        if not self.classes:\n",
        "            # Get list of distinct labels that exist in the view\n",
        "            self.classes = self.samples.distinct(\n",
        "                \"%s.detections.label\" % gt_field\n",
        "            )\n",
        "\n",
        "        if self.classes[0] != \"background\":\n",
        "            self.classes = [\"background\"] + self.classes\n",
        "\n",
        "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        sample = self.samples[img_path]\n",
        "        metadata = sample.metadata\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        area = []\n",
        "        iscrowd = []\n",
        "        detections = sample[self.gt_field].detections\n",
        "        for det in detections:\n",
        "            category_id = self.labels_map_rev[det.label]\n",
        "            coco_obj = fouc.COCOObject.from_label(\n",
        "                det, metadata, category_id=category_id,\n",
        "            )\n",
        "            x, y, w, h = coco_obj.bbox\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(coco_obj.category_id)\n",
        "            area.append(coco_obj.area)\n",
        "            iscrowd.append(coco_obj.iscrowd)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor([idx])\n",
        "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
        "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def get_classes(self):\n",
        "        return self.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aLHV0VXVW3l"
      },
      "source": [
        "The following code loads Faster-RCNN with a ResNet50 backbone from Torchvision and modifies the classifier for the number of classes we are training on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4PuSY1rWlTy"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_model(num_classes):\n",
        "    # load a model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall deepspeed\n",
        "!pip install deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAWX9XjkzCWH",
        "outputId": "cf74244c-f375-42c9-b926-717d4d9c4c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.10.13)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->deepspeed) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coKEeFj3WfhF"
      },
      "source": [
        "For this example, we are going to write a simple training loop. This function is going to take a model and our PyTorch datasets as input and use the [`train_one_epoch()`](https://github.com/pytorch/vision/blob/master/references/detection/engine.py) and [`evaluate()`](https://github.com/pytorch/vision/blob/master/references/detection/engine.py) functions from the Torchvision object detection code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX5MuzD-VX5i"
      },
      "outputs": [],
      "source": [
        "# Import functions from the torchvision references we cloned\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "def do_training(model, torch_dataset, torch_dataset_test, num_epochs=4):\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        torch_dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        torch_dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(\"Using device %s\" % device)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                    step_size=3,\n",
        "                                                    gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        # evaluate on the test dataset\n",
        "        evaluate(model, data_loader_test, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.youtube.com/watch?v=ia_mKuZ-iTE"
      ],
      "metadata": {
        "id": "f0Va3-zs9tpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRKqLZrWV0VR"
      },
      "source": [
        "## FiftyOne views and datasets\n",
        "\n",
        "One of the primary ways of interacting with your FiftyOne dataset is through different [views](https://voxel51.com/docs/fiftyone/user_guide/using_views.html) into your dataset. These are constructed by applying operations like filtering, sorting, slicing, etc, that result in a specific view into certain labels/samples of your dataset. These operations make it easier to experiment with different subsets of data and continue to finetune your dataset to train better models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqU6Ckq4WKHK"
      },
      "source": [
        "For example, cluttered images make it difficult for models to localize objects. We can use FiftyOne to create a view containing only samples with more than, say, 10 objects. You can perform the same operations on views as datasets, so we can create an instance of our PyTorch dataset from this view:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLACOukJFUxd"
      },
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "busy_view = fo_dataset.match(F(\"ground_truth.detections\").length() > 10)\n",
        "\n",
        "busy_torch_dataset = FiftyOneTorchDataset(busy_view)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "xVs_jYcLFXII",
        "outputId": "03bef9f1-8a15-46eb-c817-46bca1c01039"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "body, html {\n",
              "  margin: 0;\n",
              "  padding: 0;\n",
              "  width: 100%;\n",
              "}\n",
              "\n",
              "#focontainer-51d4f23e-7a99-4fb3-96ab-fa1e6b879392 {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-51d4f23e-7a99-4fb3-96ab-fa1e6b879392 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-51d4f23e-7a99-4fb3-96ab-fa1e6b879392:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-51d4f23e-7a99-4fb3-96ab-fa1e6b879392 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-51d4f23e-7a99-4fb3-96ab-fa1e6b879392\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-51d4f23e-7a99-4fb3-96ab-fa1e6b879392\">\n",
              "      <button id=\"foactivate-51d4f23e-7a99-4fb3-96ab-fa1e6b879392\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "session.view = busy_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKsE_7TOWXBE"
      },
      "source": [
        "Another example is if we want to train a model that is used primarily for road vehicle detection. We can easily create training and testing views (and corresponding PyTorch datasets) that only contain the classes car, truck, and bus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TELK0NWmWrMT",
        "outputId": "0240774d-2439-43c4-d31b-7113b80427d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "707\n"
          ]
        }
      ],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "vehicles_list = [\"car\", \"truck\", \"bus\"]\n",
        "vehicles_view = fo_dataset.filter_labels(\"ground_truth\",\n",
        "        F(\"label\").is_in(vehicles_list))\n",
        "\n",
        "print(len(vehicles_view))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "COjr-9RvErG-",
        "outputId": "b06c9ddf-da95-41d2-a193-93b481a74127"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "body, html {\n",
              "  margin: 0;\n",
              "  padding: 0;\n",
              "  width: 100%;\n",
              "}\n",
              "\n",
              "#focontainer-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88 {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88 {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88 {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88\">\n",
              "      <button id=\"foactivate-c94b7cda-cff8-4dd9-a681-3d5cff7b3a88\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "session.view = vehicles_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BjUPS6FWndn"
      },
      "outputs": [],
      "source": [
        "# From the torchvision references we cloned\n",
        "import transforms as T\n",
        "\n",
        "train_transforms = T.Compose([T.ToTensor(), T.RandomHorizontalFlip(0.5)])\n",
        "test_transforms = T.Compose([T.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g6hT1XkWsrn"
      },
      "outputs": [],
      "source": [
        "# split the dataset in train and test set\n",
        "train_view = vehicles_view.take(500, seed=51)\n",
        "test_view = vehicles_view.exclude([s.id for s in train_view])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506vhQ9gZmWX"
      },
      "outputs": [],
      "source": [
        "# use our dataset and defined transformations\n",
        "torch_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
        "        classes=vehicles_list)\n",
        "torch_dataset_test = FiftyOneTorchDataset(test_view, test_transforms,\n",
        "        classes=vehicles_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5je6lVBWz5r"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "In this section, we use the functions and datasets we defined above to initialize, train, and evaluate a model continuing with the vehicle example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHa6KRbEWuxz",
        "outputId": "5c5d222b-1321-4993-89b5-efb087096897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = get_model(len(vehicles_list)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "o3xZ6YMjWwY5",
        "outputId": "913d2d41-618a-4d36-a382-212c28bb3e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Epoch: [0]  [  0/250]  eta: 0:35:00  lr: 0.000025  loss: 1.9943 (1.9943)  loss_classifier: 1.3978 (1.3978)  loss_box_reg: 0.5471 (0.5471)  loss_objectness: 0.0436 (0.0436)  loss_rpn_box_reg: 0.0057 (0.0057)  time: 8.4038  data: 0.3266  max mem: 2351\n",
            "Epoch: [0]  [ 10/250]  eta: 0:05:08  lr: 0.000226  loss: 1.6900 (1.6729)  loss_classifier: 1.2447 (1.1790)  loss_box_reg: 0.4504 (0.4553)  loss_objectness: 0.0200 (0.0269)  loss_rpn_box_reg: 0.0085 (0.0117)  time: 1.2849  data: 0.0378  max mem: 3413\n",
            "Epoch: [0]  [ 20/250]  eta: 0:03:40  lr: 0.000426  loss: 1.1354 (1.3434)  loss_classifier: 0.7016 (0.8486)  loss_box_reg: 0.4504 (0.4355)  loss_objectness: 0.0203 (0.0335)  loss_rpn_box_reg: 0.0089 (0.0259)  time: 0.5879  data: 0.0120  max mem: 3597\n",
            "Epoch: [0]  [ 30/250]  eta: 0:03:03  lr: 0.000627  loss: 0.9361 (1.1677)  loss_classifier: 0.4057 (0.6941)  loss_box_reg: 0.3928 (0.4105)  loss_objectness: 0.0360 (0.0372)  loss_rpn_box_reg: 0.0178 (0.0259)  time: 0.5834  data: 0.0143  max mem: 3597\n",
            "Epoch: [0]  [ 40/250]  eta: 0:02:39  lr: 0.000827  loss: 0.7342 (1.0640)  loss_classifier: 0.3136 (0.5919)  loss_box_reg: 0.3853 (0.4044)  loss_objectness: 0.0248 (0.0347)  loss_rpn_box_reg: 0.0206 (0.0330)  time: 0.5488  data: 0.0113  max mem: 3839\n",
            "Epoch: [0]  [ 50/250]  eta: 0:02:23  lr: 0.001028  loss: 0.6608 (0.9952)  loss_classifier: 0.2386 (0.5232)  loss_box_reg: 0.3853 (0.4034)  loss_objectness: 0.0249 (0.0380)  loss_rpn_box_reg: 0.0129 (0.0307)  time: 0.5371  data: 0.0096  max mem: 3839\n",
            "Epoch: [0]  [ 60/250]  eta: 0:02:12  lr: 0.001229  loss: 0.7335 (0.9485)  loss_classifier: 0.2275 (0.4722)  loss_box_reg: 0.3976 (0.4004)  loss_objectness: 0.0320 (0.0404)  loss_rpn_box_reg: 0.0257 (0.0355)  time: 0.5623  data: 0.0120  max mem: 3839\n",
            "Epoch: [0]  [ 70/250]  eta: 0:02:01  lr: 0.001429  loss: 0.6664 (0.9044)  loss_classifier: 0.2037 (0.4384)  loss_box_reg: 0.3626 (0.3937)  loss_objectness: 0.0268 (0.0386)  loss_rpn_box_reg: 0.0257 (0.0337)  time: 0.5606  data: 0.0114  max mem: 3839\n",
            "Epoch: [0]  [ 80/250]  eta: 0:01:53  lr: 0.001630  loss: 0.6166 (0.8875)  loss_classifier: 0.2154 (0.4181)  loss_box_reg: 0.3677 (0.3995)  loss_objectness: 0.0227 (0.0377)  loss_rpn_box_reg: 0.0125 (0.0322)  time: 0.5731  data: 0.0107  max mem: 3839\n",
            "Epoch: [0]  [ 90/250]  eta: 0:01:44  lr: 0.001830  loss: 0.6000 (0.8438)  loss_classifier: 0.2094 (0.3917)  loss_box_reg: 0.2970 (0.3788)  loss_objectness: 0.0379 (0.0406)  loss_rpn_box_reg: 0.0161 (0.0327)  time: 0.5816  data: 0.0131  max mem: 3839\n",
            "Epoch: [0]  [100/250]  eta: 0:01:36  lr: 0.002031  loss: 0.4005 (0.8067)  loss_classifier: 0.1647 (0.3700)  loss_box_reg: 0.2012 (0.3639)  loss_objectness: 0.0379 (0.0405)  loss_rpn_box_reg: 0.0205 (0.0323)  time: 0.5537  data: 0.0112  max mem: 3839\n",
            "Epoch: [0]  [110/250]  eta: 0:01:29  lr: 0.002232  loss: 0.4660 (0.7861)  loss_classifier: 0.1737 (0.3533)  loss_box_reg: 0.2464 (0.3563)  loss_objectness: 0.0222 (0.0421)  loss_rpn_box_reg: 0.0222 (0.0344)  time: 0.5657  data: 0.0105  max mem: 3839\n",
            "Epoch: [0]  [120/250]  eta: 0:01:22  lr: 0.002432  loss: 0.4751 (0.7605)  loss_classifier: 0.1778 (0.3391)  loss_box_reg: 0.2485 (0.3464)  loss_objectness: 0.0176 (0.0406)  loss_rpn_box_reg: 0.0218 (0.0344)  time: 0.5779  data: 0.0112  max mem: 3839\n",
            "Epoch: [0]  [130/250]  eta: 0:01:14  lr: 0.002633  loss: 0.3907 (0.7367)  loss_classifier: 0.1486 (0.3251)  loss_box_reg: 0.2067 (0.3362)  loss_objectness: 0.0150 (0.0417)  loss_rpn_box_reg: 0.0185 (0.0337)  time: 0.5551  data: 0.0095  max mem: 3839\n",
            "Epoch: [0]  [140/250]  eta: 0:01:08  lr: 0.002833  loss: 0.4366 (0.7168)  loss_classifier: 0.1431 (0.3132)  loss_box_reg: 0.2167 (0.3294)  loss_objectness: 0.0253 (0.0409)  loss_rpn_box_reg: 0.0176 (0.0333)  time: 0.5451  data: 0.0103  max mem: 3839\n",
            "Epoch: [0]  [150/250]  eta: 0:01:01  lr: 0.003034  loss: 0.4443 (0.6958)  loss_classifier: 0.1388 (0.3018)  loss_box_reg: 0.2138 (0.3219)  loss_objectness: 0.0223 (0.0397)  loss_rpn_box_reg: 0.0129 (0.0324)  time: 0.5709  data: 0.0110  max mem: 3839\n",
            "Epoch: [0]  [160/250]  eta: 0:00:55  lr: 0.003235  loss: 0.4443 (0.6869)  loss_classifier: 0.1400 (0.2944)  loss_box_reg: 0.2034 (0.3181)  loss_objectness: 0.0192 (0.0401)  loss_rpn_box_reg: 0.0142 (0.0343)  time: 0.5573  data: 0.0095  max mem: 3839\n",
            "Epoch: [0]  [170/250]  eta: 0:00:48  lr: 0.003435  loss: 0.3226 (0.6680)  loss_classifier: 0.1399 (0.2855)  loss_box_reg: 0.1474 (0.3092)  loss_objectness: 0.0229 (0.0402)  loss_rpn_box_reg: 0.0114 (0.0331)  time: 0.5482  data: 0.0116  max mem: 3931\n",
            "Epoch: [0]  [180/250]  eta: 0:00:42  lr: 0.003636  loss: 0.2911 (0.6550)  loss_classifier: 0.1197 (0.2776)  loss_box_reg: 0.1164 (0.3010)  loss_objectness: 0.0298 (0.0434)  loss_rpn_box_reg: 0.0066 (0.0330)  time: 0.5970  data: 0.0122  max mem: 3931\n",
            "Epoch: [0]  [190/250]  eta: 0:00:36  lr: 0.003837  loss: 0.3183 (0.6406)  loss_classifier: 0.1221 (0.2718)  loss_box_reg: 0.1119 (0.2931)  loss_objectness: 0.0357 (0.0434)  loss_rpn_box_reg: 0.0102 (0.0323)  time: 0.5933  data: 0.0091  max mem: 3931\n",
            "Epoch: [0]  [200/250]  eta: 0:00:30  lr: 0.004037  loss: 0.3705 (0.6367)  loss_classifier: 0.1681 (0.2669)  loss_box_reg: 0.1527 (0.2907)  loss_objectness: 0.0327 (0.0442)  loss_rpn_box_reg: 0.0156 (0.0348)  time: 0.5774  data: 0.0135  max mem: 3931\n",
            "Epoch: [0]  [210/250]  eta: 0:00:24  lr: 0.004238  loss: 0.3705 (0.6238)  loss_classifier: 0.1593 (0.2609)  loss_box_reg: 0.1608 (0.2852)  loss_objectness: 0.0302 (0.0434)  loss_rpn_box_reg: 0.0192 (0.0343)  time: 0.5702  data: 0.0135  max mem: 3931\n",
            "Epoch: [0]  [220/250]  eta: 0:00:18  lr: 0.004438  loss: 0.3687 (0.6168)  loss_classifier: 0.1447 (0.2567)  loss_box_reg: 0.1476 (0.2810)  loss_objectness: 0.0310 (0.0443)  loss_rpn_box_reg: 0.0143 (0.0348)  time: 0.5639  data: 0.0087  max mem: 3931\n",
            "Epoch: [0]  [230/250]  eta: 0:00:12  lr: 0.004639  loss: 0.4862 (0.6108)  loss_classifier: 0.1733 (0.2536)  loss_box_reg: 0.1931 (0.2779)  loss_objectness: 0.0344 (0.0444)  loss_rpn_box_reg: 0.0163 (0.0350)  time: 0.5769  data: 0.0124  max mem: 3931\n",
            "Epoch: [0]  [240/250]  eta: 0:00:05  lr: 0.004840  loss: 0.4641 (0.6018)  loss_classifier: 0.1790 (0.2503)  loss_box_reg: 0.1931 (0.2736)  loss_objectness: 0.0287 (0.0436)  loss_rpn_box_reg: 0.0175 (0.0344)  time: 0.5569  data: 0.0121  max mem: 3931\n",
            "Epoch: [0]  [249/250]  eta: 0:00:00  lr: 0.005000  loss: 0.3497 (0.5954)  loss_classifier: 0.1730 (0.2472)  loss_box_reg: 0.1602 (0.2705)  loss_objectness: 0.0287 (0.0434)  loss_rpn_box_reg: 0.0118 (0.0343)  time: 0.5582  data: 0.0090  max mem: 3931\n",
            "Epoch: [0] Total time: 0:02:29 (0.5992 s / it)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-88b95e98dd26>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-8967fc4f512b>\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(model, torch_dataset, torch_dataset_test, num_epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mevaluator_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mcoco_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mevaluator_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mevaluator_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/coco_eval.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miou_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miou_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mcoco_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mcoco_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miou_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/coco_eval.py\u001b[0m in \u001b[0;36mloadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# print('Loading and preparing results...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# tic = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"
          ]
        }
      ],
      "source": [
        "do_training(model, torch_dataset, torch_dataset_test, num_epochs=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj3vLT1eXFnk"
      },
      "source": [
        "One of the main draws of FiftyOne is the ability to find failure modes of your model. The [built-in evaluation protocols](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html#evaluating-models) will help you find where your model got things right and where it got things wrong. Before we can evaluate the model, we need to run it on our test set and store the results in FiftyOne. Doing this is fairly simple and just requires us to run inference for the test images, get their corresponding [FiftyOne samples](https://voxel51.com/docs/fiftyone/user_guide/using_datasets.html#samples), and add a new field called `predictions` to each sample to store the detections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpg0WNh_WyzQ"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "def convert_torch_predictions(preds, det_id, s_id, w, h, classes):\n",
        "    # Convert the outputs of the torch model into a FiftyOne Detections object\n",
        "    dets = []\n",
        "    for bbox, label, score in zip(\n",
        "        preds[\"boxes\"].cpu().detach().numpy(),\n",
        "        preds[\"labels\"].cpu().detach().numpy(),\n",
        "        preds[\"scores\"].cpu().detach().numpy()\n",
        "    ):\n",
        "        # Parse prediction into FiftyOne Detection object\n",
        "        x0,y0,x1,y1 = bbox\n",
        "        coco_obj = fouc.COCOObject(det_id, s_id, int(label), [x0, y0, x1-x0, y1-y0])\n",
        "        det = coco_obj.to_detection((w,h), classes)\n",
        "        det[\"confidence\"] = float(score)\n",
        "        dets.append(det)\n",
        "        det_id += 1\n",
        "\n",
        "    detections = fo.Detections(detections=dets)\n",
        "\n",
        "    return detections, det_id\n",
        "\n",
        "def add_detections(model, torch_dataset, view, field_name=\"predictions\"):\n",
        "    # Run inference on a dataset and add results to FiftyOne\n",
        "    torch.set_num_threads(1)\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(\"Using device %s\" % device)\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    image_paths = torch_dataset.img_paths\n",
        "    classes = torch_dataset.classes\n",
        "    det_id = 0\n",
        "\n",
        "    with fo.ProgressBar() as pb:\n",
        "        for img, targets in pb(torch_dataset):\n",
        "            # Get FiftyOne sample indexed by unique image filepath\n",
        "            img_id = int(targets[\"image_id\"][0])\n",
        "            img_path = image_paths[img_id]\n",
        "            sample = view[img_path]\n",
        "            s_id = sample.id\n",
        "            w = sample.metadata[\"width\"]\n",
        "            h = sample.metadata[\"height\"]\n",
        "\n",
        "            # Inference\n",
        "            preds = model(img.unsqueeze(0).to(device))[0]\n",
        "\n",
        "            detections, det_id = convert_torch_predictions(\n",
        "                preds,\n",
        "                det_id,\n",
        "                s_id,\n",
        "                w,\n",
        "                h,\n",
        "                classes,\n",
        "            )\n",
        "\n",
        "            sample[field_name] = detections\n",
        "            sample.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkzG1i3AW1O7"
      },
      "outputs": [],
      "source": [
        "add_detections(model, torch_dataset_test, fo_dataset, field_name=\"predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEw_hYOzW3yz"
      },
      "outputs": [],
      "source": [
        "results = fo.evaluate_detections(\n",
        "    test_view,\n",
        "    \"predictions\",\n",
        "    classes=[\"car\", \"bus\", \"truck\"],\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oet0wg9XVlA"
      },
      "source": [
        "The [DetectionResults](https://voxel51.com/docs/fiftyone/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) object that is returned stores information like the mAP and contains functions that let you [plot confusion matrices, precision-recall curves, and more](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html). Also, these evaluation runs are tracked in FiftyOne and can be managed through functions like [list_evaluations()](https://voxel51.com/docs/fiftyone/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.list_evaluations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uYdXrhgYdJ_"
      },
      "outputs": [],
      "source": [
        "results.mAP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcJBOM76aJPR"
      },
      "outputs": [],
      "source": [
        "results.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nddFfGSnXo7i"
      },
      "source": [
        "By default, objects are only matched with other objects of the same class. In order to get an interesting confusion matrix, we need to match interclass objects by setting `classwise=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_53aCMna2Vt"
      },
      "outputs": [],
      "source": [
        "results_interclass = fo.evaluate_detections(\n",
        "    test_view,\n",
        "    \"predictions\",\n",
        "    classes=[\"car\", \"bus\", \"truck\"],\n",
        "    compute_mAP=True,\n",
        "    classwise=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbqf-NuAZ7Ps"
      },
      "outputs": [],
      "source": [
        "results_interclass.plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG_n8iWMX2vn"
      },
      "source": [
        "Note that there appears to be confusion between car and truck classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElSV7tTbYKLr"
      },
      "source": [
        "The [detection evaluation](https://voxel51.com/docs/fiftyone/user_guide/evaluation.html#detections) also added the attributes `eval_fp`, `eval_tp`, and `eval_fn` to every predicted detection indicating if it is a false positive, true positive, or false negative.\n",
        "Let's create a view to find the worst samples by sorting by `eval_fp` using the [FiftyOne App](https://voxel51.com/docs/fiftyone/user_guide/app.html) to visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm4Z52rd8AC1"
      },
      "outputs": [],
      "source": [
        "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0QENj62YuTQ"
      },
      "source": [
        "Looking through some of these samples, we can see the confusion between the classes \"car\" and \"truck\" that we found earlier. However, this seems to be, at least in part, due to annotation errors on vans and SUVs where they are interchangably labeled as \"car\" and \"truck\". The example below shows an SUV annotated as \"truck\" but predicted as \"car\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njLG0l5K-ucV"
      },
      "outputs": [],
      "source": [
        "session.view = test_view.sort_by(\"eval_fp\", reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReXDVFgLZLtf"
      },
      "source": [
        "It would be best to get this [data reannotated to fix these mistakes](https://towardsdatascience.com/managing-annotation-mistakes-with-fiftyone-and-labelbox-fc6e87b51102), but in the meantime, we can easily remedy this by simply creating a new view that remaps the labels `car`, `truck`, and `bus` all to `vehicle` and then retraining the model with that. This is only possible because we are backing our data in FiftyOne and loading views into PyTorch as needed. Without FiftyOne, the PyTorch dataset class or the underlying data would need to be changed to remap these classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9php90KrW9ST"
      },
      "outputs": [],
      "source": [
        "# map labels to single vehicle class\n",
        "vehicles_map = {c: \"vehicle\" for c in vehicles_list}\n",
        "\n",
        "train_map_view = train_view.map_labels(\"ground_truth\", vehicles_map)\n",
        "test_map_view = test_view.map_labels(\"ground_truth\", vehicles_map)\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "torch_map_dataset = FiftyOneTorchDataset(train_map_view, train_transforms)\n",
        "torch_map_dataset_test = FiftyOneTorchDataset(test_map_view, test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynRCHQv8XB_v"
      },
      "outputs": [],
      "source": [
        "# Only 2 classes (background and vehicle)\n",
        "vehicle_model = get_model(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCM_g65EXDzS"
      },
      "outputs": [],
      "source": [
        "do_training(vehicle_model, torch_map_dataset, torch_map_dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-mrVOl4XFbp"
      },
      "outputs": [],
      "source": [
        "add_detections(vehicle_model, torch_map_dataset_test, test_map_view, field_name=\"vehicle_predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfd3xvhaXhl_"
      },
      "outputs": [],
      "source": [
        "vehicle_results = fo.evaluate_detections(\n",
        "    test_map_view,\n",
        "    \"vehicle_predictions\",\n",
        "    classes=[\"vehicle\"],\n",
        "    eval_key=\"vehicle_eval\",\n",
        "    compute_mAP=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFvddH3rk0NR"
      },
      "outputs": [],
      "source": [
        "vehicle_results.mAP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwbhq18sk1PL"
      },
      "outputs": [],
      "source": [
        "vehicle_results.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJMAkJbWZ_u1"
      },
      "source": [
        "Due to our ability to easily visualize and manage our dataset with FiftyOne, we were able to spot and take action on a dataset issue that would otherwise have gone unnoticed if we only concerned ourselves with dataset-wide evaluation metrics and fixed dataset representations. Through these efforts, we managed to increase the mAP of the model to 43%.\n",
        "\n",
        "Even though this example workflow may not work in all situations, this kind of class-merging strategy can be effective in cases where more fine-grained discrimination is not called for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0ZyjvCaaMTV"
      },
      "source": [
        "## Summary\n",
        "\n",
        "PyTorch and related frameworks provide quick and easy methods to bootstrap your model development and training pipelines. However, they largely overlook the need to massage and finetune datasets to efficiently improve performance. FiftyOne makes it easy to load your datasets into a flexible format that works well with existing tools allowing you to provide better data for training and testing. As they say, \"garbage in, garbage out\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7W7IqaYoePZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}